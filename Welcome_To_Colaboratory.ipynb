{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rameshver43/Practice/blob/master/Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "_YgJ68fy28bV",
        "outputId": "41bae78e-e869-4489-a1b3-a106e726868d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/7.2 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m6.5/7.2 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def preprocess_text(text, tokenizer):\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "    return inputs\n",
        "\n",
        "def train_model(model, train_inputs, train_labels, num_epochs=10, batch_size=16):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "    model.to(device)\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'],\n",
        "                                                  train_inputs['attention_mask'],\n",
        "                                                  train_labels)\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            input_ids, attention_mask, labels = batch\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {total_loss:.4f}')\n",
        "\n",
        "def classify_sentence(model, sentence, tokenizer):\n",
        "    model.eval()\n",
        "    input_data = preprocess_text(sentence, tokenizer)\n",
        "    input_ids = input_data['input_ids'].to(device)\n",
        "    attention_mask = input_data['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predicted_label = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "\n",
        "model_name = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
        "\n",
        "    # Example training data (paragraphs)\n",
        "train_paragraphs = [\n",
        "        \"Yoga is an ancient practice that originated in India thousands of years ago. It encompasses a holistic approach to physical and mental well-being, combining physical postures (asanas), breathing exercises (pranayama), and meditation techniques. The practice of yoga aims to harmonize the body, mind, and spirit, promoting balance, flexibility, strength, and inner peace. Through the regular practice of yoga, individuals can experience a myriad of benefits. Physically, yoga helps improve flexibility, increase muscle tone and strength, and enhance overall body awareness. It can also alleviate chronic pain, reduce the risk of injury, and improve posture and alignment.Mentally and emotionally, yoga provides a space for mindfulness and self-reflection. It can reduce stress, anxiety, and depression by promoting relaxation and calming the mind. Regular yoga practice cultivates focus, concentration, and mental clarity. It also encourages self-acceptance, self-compassion, and a deeper connection with oneself and others. Furthermore, yoga offers a spiritual dimension, allowing practitioners to explore and deepen their spiritual connection. It encourages the development of inner wisdom, compassion, and gratitude. Yoga philosophy encompasses principles such as non-violence (ahimsa), truthfulness (satya), contentment (santosha), and self-discipline (tapas). Yoga is a versatile practice that offers various styles and approaches to suit individual preferences and needs. Hatha yoga focuses on physical postures and breath control, while Vinyasa yoga emphasizes flowing movements coordinated with breath. Other styles like Ashtanga, Kundalini, and Iyengar each have their unique characteristics and benefits. Practicing yoga is accessible to people of all ages and fitness levels. It can be adapted to accommodate different body types, abilities, and limitations. Whether practiced in a group setting or at home, yoga provides an opportunity to nurture oneself, find inner balance, and cultivate overall well-being.\",\n",
        "        \"Football, also known as soccer, is a popular sport played worldwide. It involves two teams competing against each other to score goals by kicking a ball into the opposing team's net. Football requires physical agility, speed, endurance, and teamwork. The game of football consists of various positions, including forwards, midfielders, defenders, and goalkeepers. Each position has its unique role and responsibilities. The forwards primarily focus on scoring goals, while midfielders play a critical role in controlling the game and facilitating transitions between defense and attack. Defenders aim to prevent the opposing team from scoring, and goalkeepers protect the goal from incoming shots. Football matches are played on a rectangular field with two goals at opposite ends. The objective is to move the ball across the field using various passing, dribbling, and shooting techniques. Strategies and tactics are employed to outwit the opposing team and create scoring opportunities.Beyond the physical aspects, football fosters camaraderie, sportsmanship, and teamwork among players. It promotes discipline, dedication, and perseverance. Football matches often evoke a sense of passion, excitement, and unity among fans and supporters. The popularity of football has led to the establishment of professional leagues and international tournaments, such as the FIFA World Cup and UEFA Champions League. These events showcase the highest level of competition and bring people from different nations together through their shared love for the sport. Football enthusiasts engage in watching matches, playing in local teams, and participating in fantasy football leagues. The sport has a significant cultural impact and serves as a source of entertainment, social connection, and inspiration for millions of people worldwide. Basketball is a popular sport played around the world. It involves two teams competing to score points by shooting a ball into the opposing team's basket. While basketball requires physical fitness and coordination, it is not directly related to yoga.\"\n",
        "    ]\n",
        "train_labels = [1, 0]  # 1 for yoga-related, 0 for not yoga-related\n",
        "\n",
        "    # Preprocess and train the model\n",
        "train_inputs = preprocess_text(train_paragraphs, tokenizer)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_model(model, train_inputs, train_labels)\n",
        "\n",
        "# model.save_pretrained(\"saved_model\")\n",
        "# tokenizer.save_pretrained(\"saved_tokenizer\")\n",
        "test_sentence = \"Yoga is an ancient practice that originated in India thousands of years ago.\"\n",
        "predicted_label = classify_sentence(model, test_sentence, tokenizer)\n",
        "print(predicted_label)  # 1 (yoga-related)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDMgAY6h1cPE",
        "outputId": "baee4bc9-3184-4604-be8c-e66da421482e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 0.7141\n",
            "Epoch 2/10 - Loss: 0.6077\n",
            "Epoch 3/10 - Loss: 0.6448\n",
            "Epoch 4/10 - Loss: 0.5447\n",
            "Epoch 5/10 - Loss: 0.5424\n",
            "Epoch 6/10 - Loss: 0.5912\n",
            "Epoch 7/10 - Loss: 0.4894\n",
            "Epoch 8/10 - Loss: 0.4220\n",
            "Epoch 9/10 - Loss: 0.4440\n",
            "Epoch 10/10 - Loss: 0.4933\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"saved_model\")\n",
        "tokenizer.save_pretrained(\"saved_tokenizer\")"
      ],
      "metadata": {
        "id": "ceGpVLxm6sN5",
        "outputId": "2eac24df-9b93-40bd-b155-46efdae4c707",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('saved_tokenizer/tokenizer_config.json',\n",
              " 'saved_tokenizer/special_tokens_map.json',\n",
              " 'saved_tokenizer/vocab.txt',\n",
              " 'saved_tokenizer/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentence(model, sentence, tokenizer):\n",
        "    model.eval()\n",
        "    input_data = preprocess_text(sentence, tokenizer)\n",
        "    input_ids = input_data['input_ids'].to(device)\n",
        "    attention_mask = input_data['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        predicted_label = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"saved_model\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"saved_tokenizer\")\n",
        "# Example test sentence\n",
        "test_sentence = \"Play any video for relaxation yoga\"\n",
        "predicted_label = classify_sentence(model, test_sentence, tokenizer)\n",
        "print(predicted_label)  # 1 (yoga-related)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IdCOak62VIi",
        "outputId": "1c67f49d-909e-4e50-a5b8-914f466a2149"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}