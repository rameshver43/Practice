{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQciroQuCb7UUoV+EcaTEf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rameshver43/Practice/blob/master/neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network**"
      ],
      "metadata": {
        "id": "3IH1DlPZpjz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Differents way to create neural networks:\n",
        "\n",
        "1.   **Building from scratch**: This approach involves manually implementing the forward and backward propagation algorithms using a mathematical library like NumPy or a deep learning framework like TensorFlow or PyTorch. It provides the highest level of control but requires more effort and expertise.\n",
        "2.   **Using deep learning frameworks**: Frameworks like TensorFlow, PyTorch, Keras, and Caffe provide high-level APIs that abstract away many implementation details, making it easier to build and train neural networks. These frameworks offer a variety of pre-defined layers, activation functions, and optimization algorithms that you can use to create your model.\n",
        "3.   **Transfer learning**: Transfer learning involves leveraging pre-trained neural network models, such as those trained on large-scale image datasets like ImageNet. By utilizing the weights and architectures of these models, you can either fine-tune them on your specific task or use them as feature extractors by freezing the pre-trained layers and adding your own custom layers on top.\n",
        "4.   **AutoML tools**: Automated Machine Learning (AutoML) platforms and tools, such as Google AutoML, H2O.ai, and Auto-Keras, provide a higher level of abstraction, automating the process of designing and training neural networks. These tools often use techniques like neural architecture search (NAS) to automatically discover the optimal network architecture for a given task.\n",
        "5.  ***Using pre-built models***: Many deep learning frameworks offer pre-built models for specific tasks, such as image classification (e.g., VGG, ResNet) or natural language processing (e.g., BERT, GPT). These models are typically trained on large-scale datasets and can be readily used or fine-tuned for similar tasks without requiring extensive network design.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_3WORIm1p1QA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different examples of creating models using TensorFlow:\n",
        "\n",
        "\n",
        "*   **Sequential Model**: The Sequential model is a linear stack of layers. It's the simplest way to create a model in TensorFlow, suitable for most common use cases.\n",
        "\n"
      ],
      "metadata": {
        "id": "1bR53NiAqfSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "jpn5BPUOpnyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Functional API**: The Functional API allows for more complex model architectures, including models with shared layers or multiple inputs/outputs."
      ],
      "metadata": {
        "id": "J3pi-9tBqziF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "input_1 = tf.keras.Input(shape=(input_dim,))\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(input_1)\n",
        "x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
        "output = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=input_1, outputs=output)\n"
      ],
      "metadata": {
        "id": "GY88E4cQq4Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Subclassing Model**: Subclassing the tf.keras.Model class provides the highest level of flexibility, allowing you to define custom forward pass computations and create complex architectures"
      ],
      "metadata": {
        "id": "Yr3iaG_Wq6eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(10, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return x\n",
        "\n",
        "model = MyModel()\n"
      ],
      "metadata": {
        "id": "LfmX1MHUq9_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-trained Models**: TensorFlow provides pre-trained models that you can load and use directly for tasks like image classification, object detection, and natural language processing."
      ],
      "metadata": {
        "id": "D1YfYz_urAQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False)\n",
        "# Add custom layers on top of the pre-trained model\n",
        "x = base_model.output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "output = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=base_model.input, outputs=output)\n"
      ],
      "metadata": {
        "id": "siUDtrEHrDX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main difference between creating a model using the Sequential model and using the Functional API in TensorFlow lies in the flexibility and complexity of the model architectures that can be constructed.\n",
        "\n",
        "\n",
        "**Sequential Model**: The Sequential model is a linear stack of layers, where each layer has a single input tensor and produces a single output tensor. It is suitable for building simple models with a straightforward flow of data, where each layer feeds into the next sequentially.\n",
        "With the Sequential model, you define the model architecture by adding layers in sequence using the .add() method. It is the simplest and most commonly used way to create models in TensorFlow, especially for tasks like image classification or simple feedforward neural networks.\n",
        "The Sequential model is easy to use and understand, but it may not be suitable for more complex architectures with multiple inputs/outputs or shared layers.\n",
        "\n",
        "**Functional API**: The Functional API in TensorFlow provides a more flexible and powerful way to create models, allowing for complex architectures with multiple inputs, multiple outputs, and shared layers.\n",
        "With the Functional API, you define the model architecture by explicitly defining the input tensors and connecting layers using function calls. Each layer is treated as a function that takes inputs and produces outputs. This enables you to create models with branching, merging, or skipping connections.The Functional API allows for more intricate model designs, such as multi-task learning models, models with residual connections, or models with skip connections. It provides greater control over the flow of data through the model.Additionally, the Functional API allows you to create models that share layers, where the same layer can be used multiple times in different parts of the model. This is particularly useful in scenarios like siamese networks or when building complex models with subnetworks."
      ],
      "metadata": {
        "id": "kOanyg-krGAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The compile() method in TensorFlow is used to configure the model for training. It takes several parameters to define the optimization process, loss function, and evaluation metrics. Here is a detailed explanation of the parameters:\n",
        "\n"
      ],
      "metadata": {
        "id": "JWBzlgGHrXz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=None,\n",
        "    metrics=None,\n",
        "    loss_weights=None,\n",
        "    weighted_metrics=None,\n",
        "    run_eagerly=None\n",
        ")\n"
      ],
      "metadata": {
        "id": "z2IMhJJNriuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**optimizer**: The optimizer parameter specifies the optimization algorithm used to update the model's weights during training. Some commonly used optimizers are 'adam', 'sgd', 'rmsprop', and 'adagrad'. You can also use custom optimizer instances from the tf.keras.optimizers module or specify optimizer parameters such as learning rate.\n",
        "\n",
        "**loss**: The loss parameter defines the loss function to be minimized during training. It quantifies how well the model predicts the output compared to the true target values. Common loss functions for different problem types include 'mean_squared_error', 'binary_crossentropy', 'categorical_crossentropy', and 'sparse_categorical_crossentropy'. You can also define custom loss functions using tf.keras.losses.\n",
        "\n",
        "**metrics**: The metrics parameter specifies the evaluation metrics to be computed during training. Metrics are used to assess the model's performance and can include accuracy, precision, recall, F1-score, and others. You can pass a single metric as a string or a list of metrics. For example: metrics='accuracy' or metrics=['accuracy', 'precision'].\n",
        "\n",
        "**loss_weights**: The loss_weights parameter allows you to assign different weights to different loss functions if your model has multiple outputs or multi-task learning. It is a list or dictionary specifying the weight for each loss. This parameter is useful when you want to emphasize or de-emphasize certain losses in the overall training objective.\n",
        "\n",
        "**weighted_metrics**: The weighted_metrics parameter allows you to specify additional metrics to evaluate during training. Similar to the metrics parameter, it can be a single metric or a list of metrics. However, weighted metrics are calculated based on the sample weights, which can be useful when the training dataset is imbalanced.\n",
        "\n",
        "**run_eagerly**: The run_eagerly parameter is a Boolean that indicates whether to execute the model eagerly or in graph mode. By default, TensorFlow executes models in graph mode for performance reasons. However, you can set run_eagerly=True to enable eager execution for debugging or dynamic models.\n",
        "\n"
      ],
      "metadata": {
        "id": "k-Y0EJFerkp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**optimizer:**\n",
        "The optimizer parameter in compile() specifies the optimization algorithm used to update the model's weights during training. TensorFlow provides various optimizers, including:\n",
        "'**adam**': Adam optimizer is a popular choice that adapts the learning rate for each parameter based on the estimates of first and second moments of the gradients.\n",
        "'**sgd**': Stochastic Gradient Descent (SGD) optimizer updates the weights based on the gradient of the loss function with respect to the weights.\n",
        "'rmsprop': RMSprop optimizer uses a moving average of squared gradients to normalize the updates.\n",
        "'**adagrad**': Adagrad optimizer adapts the learning rate based on the historical gradient information for each parameter.\n",
        "You can also use other optimizers provided by TensorFlow or even define custom optimizer instances using the tf.keras.optimizers module. For example:"
      ],
      "metadata": {
        "id": "CffXE813sBYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(optimizer=optimizer, ...)\n"
      ],
      "metadata": {
        "id": "RSou6nqOsWWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**loss**:\n",
        "The loss parameter in compile() defines the loss function to be minimized during training. The loss function quantifies the difference between the predicted output and the true target values. Different types of problems (e.g., regression, binary classification, multi-class classification) require different loss functions. Some commonly used loss functions include:\n",
        "'**mean_squared_error**': Mean Squared Error (MSE) is often used for regression problems.\n",
        "'**binary_crossentropy**': Binary Crossentropy is commonly used for binary classification problems.\n",
        "'**categorical_crossentropy**': Categorical Crossentropy is used for multi-class classification with one-hot encoded target labels.\n",
        "'**sparse_categorical_crossentropy**': Sparse Categorical Crossentropy is used for multi-class classification with integer target labels.\n",
        "You can also define custom loss functions by subclassing the tf.keras.losses.Loss class or using functional-style loss functions. For example:"
      ],
      "metadata": {
        "id": "mV5jHoeFsZEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.losses as losses\n",
        "\n",
        "loss = losses.MeanSquaredError()\n",
        "model.compile(loss=loss, ...)\n"
      ],
      "metadata": {
        "id": "TkezOodhsodp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**metrics**:\n",
        "The metrics parameter in compile() specifies the evaluation metrics to be computed during training. Metrics are used to assess the model's performance and can include accuracy, precision, recall, F1-score, and more. Some common metrics for classification tasks include:\n",
        "'**accuracy**': Computes the accuracy of the model's predictions.\n",
        "'**precision**': Calculates the precision of the model's predictions.\n",
        "'**recall**': Calculates the recall of the model's predictions.\n",
        "'**f1_score**': Computes the F1 score, which combines precision and recall.\n",
        "You can pass a single metric as a string or a list of metrics. For example:"
      ],
      "metadata": {
        "id": "PzAzuJy5sm4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(metrics=['accuracy', 'precision'])"
      ],
      "metadata": {
        "id": "zHaSh04cs2Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The fit() method is used to train a model in TensorFlow. It takes several parameters that define the training process. Let's explore the parameters in more detail:**"
      ],
      "metadata": {
        "id": "9e-kfyGhs4pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    x=None,\n",
        "    y=None,\n",
        "    batch_size=None,\n",
        "    epochs=1,\n",
        "    verbose=1,\n",
        "    callbacks=None,\n",
        "    validation_split=0.0,\n",
        "    validation_data=None,\n",
        "    shuffle=True,\n",
        "    class_weight=None,\n",
        "    sample_weight=None,\n",
        "    initial_epoch=0,\n",
        "    steps_per_epoch=None,\n",
        "    validation_steps=None,\n",
        "    validation_batch_size=None,\n",
        "    validation_freq=1,\n",
        "    max_queue_size=10,\n",
        "    workers=1,\n",
        "    use_multiprocessing=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "RUJjr-STtCLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**x**: The x parameter represents the input data for training the model. It can be a NumPy array or a TensorFlow Dataset object containing the training samples.\n",
        "\n",
        "**y**: The y parameter represents the target values or labels corresponding to the input data. It should have the same length or shape as x.\n",
        "\n",
        "**batch_size**: The batch_size parameter determines the number of samples per gradient update. It defines how many samples are processed before the model's weights are updated. Training in batches can help with memory efficiency and computational speed. If not specified, it defaults to None, meaning the default batch size is determined automatically.\n",
        "\n",
        "**epochs**: The epochs parameter specifies the number of times the model will iterate over the entire training dataset. Each epoch consists of one forward pass and one backward pass (gradient update) through the entire dataset.\n",
        "\n",
        "**verbose**: The verbose parameter controls the verbosity level during training. It determines the amount of information displayed during training. Setting verbose=1 will display a progress bar and training metrics, while verbose=0 will silence the output. There are other options for different levels of verbosity.\n",
        "\n",
        "**callbacks**: The callbacks parameter allows you to specify a list of callbacks to be applied during training. Callbacks are objects that can perform various actions at different stages of training, such as saving model checkpoints, early stopping, or custom logging.\n",
        "\n",
        "**validation_split**: The validation_split parameter specifies the fraction of the training data to be used for validation. For example, setting validation_split=0.2 will use 20% of the training data for validation, while the remaining 80% is used for training. Note that validation_split is applied after shuffling the data, if enabled.\n",
        "\n",
        "**validation_data**: The validation_data parameter allows you to provide explicit validation data for evaluating the model's performance during training. It should be a tuple of the form (x_val, y_val) representing the validation inputs and targets.\n",
        "\n",
        "**shuffle**: The shuffle parameter determines whether the training data should be shuffled at the beginning of each epoch. Shuffling the data helps introduce randomness and avoid any potential bias during training. By default, it is set to True.\n",
        "\n",
        "**class_weight**: The class_weight parameter allows you to assign different weights to different classes in case of class imbalance. It can be useful when certain classes have fewer samples and you want to give them more importance during training.\n",
        "\n",
        "These are some of the key parameters of the fit() method. There are additional parameters for more advanced use cases, such as distributed training, data parallelism, and handling large datasets. You can refer to the TensorFlow documentation for more information on these parameters and their usage."
      ],
      "metadata": {
        "id": "YAZ7DZjztNqF"
      }
    }
  ]
}